\section{Linearization Methods}
So far we have assumed that we are given the linearization parameters $\{\Linmat_n, \intcpt_n\}$ that allow us to 
approximate locally the nonlinear forward model given in Equation   \eqref{eq:linapprox}. As 
$\nonlin{\cdot}$  is a function of our latent functions approximation ${\Weights_n\feats_n}$,  
it makes sense to linearize around our current posterior estimates of these latent functions, 
$\hatlatentsn =\pWeights \feats_n$. In this section we describe two methods for linearizing 
the  forward model around these posterior estimates within our variational 
inference framework. The first method uses a Taylor series approximation and the second 
method uses a statistical linearization \citep{Geist2010} based on the unscented transform \citep{Julier2004}. 
Because of the relation of our algorithms to the Extended Gaussian Process (\egp) and 
the  Unscented Gaussian Process (\ugp) of \citet{steinberg-bonilla-nips-2014},   we refer to 
the proposed inference methods as  the Extended Kitchen Sinks (\eks) and the 
Unscented Kitchen Sinks (\uks), respectively.
%
\subsection{Taylor Series Linearization}
We can use a first order Taylor series to linearize $\nonlin{\cdot}$ at each
iteration of \eqref{eq:newt},
\begin{equation}
    \nonlin{\Weights\feats_n} \approx \nonlin{\pWeights\feats_n} +
    \Jacob{n}\brac{\Weights - \pWeights}\feats_n,
\end{equation}
where 
%$\Jacob{n} = \partial\nonlin{\latentsn} /
%\partial \latentsn  \Big|_{\latentsn = \hatlatentsn }$ and
%$\hatlatentsn =\pWeights \feats_n$. 
\begin{equation}
\Jacob{n} = \frac{ \partial\nonlin{\latentsn} } {  \partial \latentsn }  \Big|_{\latentsn = \pWeights \feats_n } \text{.}
\end{equation}
Equating coefficients with \eqref{eq:linapprox} we have that
\begin{equation}
    \Linmat_n = \Jacob{n} \qquad \text{and} \qquad \intcpt_n = 
        \nonlin{\pWeights\feats_n} - \Jacob{n}\pWeights\feats_n.
\end{equation}
%
\subsection{Statistical Linearization}
In order to have a  statistical approach to estimating the linearization parameters 
 in  Equation \eqref{eq:linapprox}, we can  use, for example, weighted least squares.  The main question 
is what ``training" data  can we use to fit the linear model? Although we can 
sample from $\Latents$ using Equation \eqref{eq:posteriorF} to generate these data, 
the unscented transform  \citep[\ut;][]{Julier2004} provides a deterministic and more elegant solution. 

The main point to notice here is that we are interested in linearizing $\nonlinf$ as a function of $\latentsn$,
 where $\latentsn$ is a $\ltdim-$dimensional random variable corresponding to the 
 $\ltdim$ latent function values at datapoint $n$. Interestingly, our choice of variational 
 distribution in Equation $\eqref{eq:linapprox}$ assumes that the joint posterior factorizes 
 across latent functions, and so does the marginal:
 \begin{align}
 	\label{eq:meanqf}
	\qrob{ \latentsn } &= \gausC{\latentsn}{ \mufn, \covfn  } \text{, with  } \mufn= \pWeights \feats_n  \text{, and}\\
	\label{eq:covqf}
	[\covfn]_{qq\prime}  &=  \feats_n\transpose  \pocov_q \feats_n  \text{ for } q = q\prime \text{  and } 0 \text{ otherwise}  \text{.}
 \end{align}
 %
 This greatly simplifies 
the computation of the \ut, which involves the definition of $2\ltdim+1$ so-called sigma-points:
 \begin{align}
\Sfunc_{0,n} & = \mufn \\
\Sfunc_{i,n} & =  \mufn +  \matcol{\sqrt{ (\ltdim + \scoef) \covfn}}{i} \quad 1 \leq i \leq \ltdim \\
\Sfunc_{i,n} & = \mufn  -  \matcol{\sqrt{ (\ltdim + \scoef) \covfn}}{i-\ltdim} \quad \ltdim < i \leq   2\ltdim
 %i=\ltdim+1, \ldots, 2\ltdim 
% \text{,} 
 \end{align}
 %
where $\matcol{\mat{A}}{i}$ denotes the $i\mth$ column of matrix $\mat{A}$ and 
$\scoef$ is a free parameter.
The corresponding forward model evaluations $\Sobs_{i,n}$, and weights $\Sw_{i,n}$:
\begin{align}
 	\Sobs_{i,n} & = \nonlinf(\Sfunc_{i,n})   \quad \text{ for } 0 \leq i \leq 2\ltdim\\\
	\Sw_{0} &= \frac{ \scoef}{\ltdim +  \scoef}, \quad
	\Sw_{i} = \frac{1}{2(\ltdim +  \scoef)}  \text{ for }     0  < i  \leq 2\ltdim  \text{,}
\end{align}
 where we note that $\scoef=1/2$ corresponds to uniform weights $\Sw_{i} = 1/(2\ltdim+1)$.
 
 Solving the weighted linear least squares problems with inputs, outputs, and weights 
$\{ \Sfunc_{i,n},  \Sobs_{i,n}, \Sw_{i} \}$ yields the solution:
\newcommand{\ybar}{\bar\outs_n}
\newcommand{ \Cross}{\mat{\Gamma}_n}
\begin{align}
	 \intcpt_n & = \ybar - \Linmat_n \pWeights \feats_n \\
	\Linmat_n &= \Cross \covfn\inv \text{,}
\end{align}
where  $\covfn$ is the diagonal matrix given in Equation \eqref{eq:covqf}, and 
 $\ybar$ and $\Cross$ are the sufficient statistics:
\begin{align}
\ybar  &=  \sum_{i=0}^{2 \ltdim} \Sw_{i} \Sobs_{i,n} \text{,} \\
 \Cross &= \sum_{i=0}^{2\ltdim} \Sw_{i} (\Sobs_{i,n} - \ybar) (\Sfunc_{i,n} - \pWeights \feats_n)\transpose \text{.}
\end{align}







































