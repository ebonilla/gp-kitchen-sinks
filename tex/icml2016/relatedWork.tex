\section{Related work}
Most previous work in the \gp community has focused on addressing  
the scalability, multi-task learning or non-linear likelihood challenges in
isolation. For example, with regards to scalability, the seminar work of \citet{quinonero2005unifying} allowed the 
community to understand most sparse approximations in \gp models from a probabilistic perspective, 
 and the   framework of \citet{titsias2009variational} has become the  underpinning  machinery
 of  most modern scalable approaches to \gp regression and classification.  
 Nevertheless, the scalability 
 problem in \gp models continues to be an intensive area of research, with recent approaches developed 
by \citet{yang-et-al-aistats-2015} with their distributed variational inference framework and 
 the scalable \gp regression and classification methods of 
 \citet{hensmangaussian} and  \citet{hensman-et-al-aistats-2015}, respectively. 
 %
 With regards to multi-output and multi-task learning,  one of the most notable approaches has 
 been developed by  \citet{alvarez-lawrence-nips-08} using the convolution formalism, although 
 their later work also focuses on developing efficient inference algorithms for such models
\citep{alvarez2011computationally}.
%
 Finally, concerning non-linear likelihoods, \citet{opper-arch-nc-2009} presented 
 the seemingly surprising (but powerful) result of estimating a full Gaussian posterior 
for models with \gp priors and general \iid  likelihoods efficiently using variational inference.  
 
Although the work by \citet{rahimi-recht-nips-2007,rahimi-recht-nips-2008} has been highly 
influential in the area of deterministic kernel machines, it is surprising that their 
random kitchen sinks (\rks) approximations had not been investigated more 
thoroughly in probabilistic kernel frameworks 
such as Gaussian process models. Only until very recently, \citet{yang-et-al-aistats-2015} 
have studied fast approximations to the kernel function, which accelerate \rks significantly. 
In particular,  \citet{yang-et-al-aistats-2015}  focus on the problem of developing scalable 
and flexible kernels for regression problems. 

Contemporary to our work,  \citet{hensman-et-al-nips-2015} and \citet{dezfouli-bonilla-nips-2015}   
have  proposed scalable approaches to inference in \gp models with general likelihoods. 
We see our extended and unscented kitchen sinks methods as alternative approaches to their
work, which builds upon the inducing-point formalism of \citet{titsias2009variational}.
A thorough comparison and evaluation of both approaches is  an interesting 
area for immediate future work.

 