\subsection{Hyperparameter Learning}
Once we have found the optimum $\pWeights$ and $\pocov_q$'s, we can optimise
the linearized $\Fengy$ (which becomes an approximation to the lower bound)
with respect to the parameters $\Lcov$, $\{ \prvar_q \}$ and $\khypers$,
assuming the features are parameterized $\feats_n = \featfunc{\ins_n,
    \khypers}$. Once we have found the optimum $\Lcov$, $\{ \prvar_q \}$,
$\khypers$, we then re-optimize for the posterior parameters  in a generalized
variational-EM like procedure.

When we have determined the optimum posterior parameters, the trace term in
Equation \eqref{eq:KL} cancels out with the only term in Equation \eqref{eq:ell_approx} 
involving $\pocov_q$, i.e.\ $QD -
\sum_q\trace{\pocov_q}/\prvar_q = 
\sum_n \sum_q \feats_n\transpose\pocov_q  \feats_n\Linvec_{nq}\transpose\Lcov\inv \Linvec_{nq}$,
to give,
\begin{multline}
    \Fengy \approx 
        - \frac{N}{2}\sbrac{P\log{2\pi} + \log\deter{\Lcov}}
    - \frac{1}{2} \sum^N_{n=1} \sbrac{
        \brac{\errorn}\transpose
        \Lcov\inv
        \brac{\errorn}} \\
        -\frac{1}{2} \sum^\ltdim_{q=1} \sbrac{
        \frac{1}{\prvar_q} \pweights_q\transpose\pweights_q
        - \log\deter{\pocov_q} + D \log \prvar_q}.
    \label{eq:modeF}
\end{multline}
Unfortunately, the optimum mean ($\pWeights\opt$) is an implicit function of both $\Lcov$ and
$\khypers$, and so in the case of the EGP, we would require second and higher
derivatives of $\nonlinf$ in order to calculate partial derivatives of
\eqref{eq:modeF} with respect to these parameters. 
%Furthermore, in the case
%of the UGP, derivatives of $\nonlinf$ may not exist\footnote{Perhaps we could
%    use repeated applications of statistical linearization to approximate the
%    derivatives.}. 
However, if we assume \bigo{10} tasks, \otdim, and a lightly
parameterized feature function, $\featfunc{\cdot}$, then we can use numerical 
optimization methods such as COBYLA and BOBYQA. 



 
