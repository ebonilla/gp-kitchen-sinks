\section{Introduction}
% 
%In this paper we are interested in inference in models with Gaussian process (\gp)
%priors and  general (non-linear) likelihoods. 

%Gaussian process models can be used as nonparametric probabilistic approaches
%to standard machine learning settings such as regression and classification
%\cite{rasmussen-williams-book}. For example, in classification problems  one
%places a  \gp prior over  latent functions, which are squashed through a
%likelihood model (such as a softmax or probit model) in order to accurately
%represent the \emph{observable} targets.  Therefore, in these situations, the
%latent functions in \gp-based models are nuisance parameters and are important
%only as a means to an end, i.e.~that of providing greater flexibility than
%their parametric counterparts.
  
 Gaussian process (\gp) models can be used as nonparametric probabilistic approaches
 to standard machine learning settings such as regression and classification
\cite{rasmussen-williams-book}, where the latent functions modeled by the \gp 
are only important as a means to an end, that of providing 
greater flexibility than their parametric counterparts.
In other application areas such as inversion problems, the latent
functions are quantities of interest themselves,  and they  are passed through
a domain-specific \emph{forward model} in order to generate the observations.
%
%As knowledge of the forward mapping from latent functions to the observations
%is available, but not the converse mapping, these applications where reasoning
%about the latent functions is as important as the purely predictive setting
%are usually referred to as \emph{inversion} problems. 

%For example, in a geophysical inversion one might be interested in inferring
%rock properties (such as wave velocities) and the structural geometry of the
%region (such as boundaries between different types of formations) from
%geophysical data (such as wave travel times). There are strong constraints on
%how the latent functions (e.g.~rock properties) relate to the given
%observations, and these constraints are implemented in the forward model.
%Clearly, in these types of applications, the goal is not only making good
%probabilistic predictions of the observables but also having good estimates of
%the posterior over  the latent functions.
%
Standard machine learning tasks and inversion problems present three key
modeling and inference challenges when having Gaussian process (\gp) priors.
The first challenge is scalability, as \gp{s} are notorious for their poor
scalability as a function of the number of training points. The second
challenge is multi-output and multi-task task learning, as required by problems
such as multi-output regression, multi-class classification or inversions over a
multi-layer geological structure (where each layer is a task).  Finally, the
third challenge is that of dealing with nonlinear likelihoods, for example in
classification, regression with non-Gaussian noise, and seismic inversion, as
the posterior over the latent functions is analytically intractable.  

\citet{steinberg-bonilla-nips-2014} have shown recently that it is possible to obtain
good posterior estimates in  \gp models using  approximations of the 
nonlinear  likelihood  via a Taylor
series expansion or via the Unscented Transform \citep{Julier2004}.  They refer
to their methods as the extended Gaussian process (\egp)
and the unscented Gaussian process (\ugp).  One of the fundamental reasons why
such linearizations are effective is because of their locality and adaptivity,
as they are constructed around the current posterior estimate, which is
iteratively updated within a variational inference procedure.
% 
While such methods are an effective way to
tackle nonlinearities in the likelihood, their approach  does not deal with the
other two challenges mentioned above, namely multi-task learning and
scalability. Indeed their method is specific to single-output problems and
inherits the cubic scalability of standard \gp{s} on the
number of training points. 
%This scalability problem is exacerbated when
%formulating  multi-task or multiple-output settings, as naively with $N$ data
%points and $M$ tasks the computational cost would be $\bigo{N^3M^3}$ in
%time, which renders their approach impractical for large datasets.  

In this paper we propose a scalable multiple-output generalization  of the
method of \citet{steinberg-bonilla-nips-2014}. We deal with multiple-output
problems by using affine transformations of the latent functions and achieve
scalability by introducing random feature approximations of the covariance
function of the Gaussian processes, in the style of
\citet{rahimi-recht-nips-2007}. Inference of all parameters and hyperparameters
is carried out using a variational inference framework, and so the kernel
learning methods introduced by \citet{yang-et-al-aistats-2015} can be
applied to our methods. Since \citet{rahimi-recht-nips-2007} refer to their
approach as Random Kitchen Sinks, we will refer to our methods as extended and
unscented kitchen sinks (\eks, \uks), when using the Taylor series
approximation or the Unscented Transform approximation in the conditional
likelihood respectively. 

Our approach naturally avoids the cubic scalability of the original \egp and
\ugp methods \citep{steinberg-bonilla-nips-2014} as a function of the number of
training points. Our algorithms' complexity is dominated by the inverse of the
feature covariance of size $D$, which has a time complexity of $\bigo{D^3}$,
where typically $D \ll N$. 

Our experiments on small-scale synthetic nonlinear inversion tasks and on a
classification task on the \usps dataset show that random feature
approximations to the \egp and the \ugp can attain similar performance to the
original methods. This applies even when using a small number of features,
hence reducing the complexity of inference significantly. Furthermore,
experiments at a larger scale on  \mnist  show that our algorithms
are competitive with recently developed approaches for inference in \gp models,
while the application of the \egp and \ugp to this task is simply infeasible.
Finally, on a multi-task (joint) nonlinear seismic inversion  problem we show
that our algorithms can recover accurate representations of the underlying
geological structure and rock properties (seismic velocities).









