\section{Introduction}
% 
%In this paper we are interested in inference in models with Gaussian process (\gp)
%priors and  general (non-linear) likelihoods. 

Gaussian process models can be used as   
nonparametric probabilistic approaches to standard machine learning settings such as 
regression and classification \cite{rasmussen-williams-book}. For example, 
in classification problems  one places a  \gp prior over  latent functions, 
which are squashed through a likelihood model (such as a softmax or probit model)
in order to accurately represent the \emph{observable} targets.  
Therefore, in these situations, the latent functions in \gp-based models 
are nuisance parameters 
and are important only as a means to an end, i.e.~that of providing greater flexibility 
than their parametric counterparts.
  
In other applications in areas such as the natural and Earth sciences, the latent functions 
are quantities of interest themselves,  and they  
are passed through a domain-specific \emph{forward model}
in order to generate the observations.  As knowledge of the forward mapping from 
latent functions to the observations is available, but not the converse mapping, these
applications where reasoning about the latent functions is as important as 
the purely predictive setting, are usually referred to as \emph{inversion} problems. 

For example, in a geophysical inversion one might be interested in inferring rock 
properties (such as wave velocities) 
and the structural geometry of the  region   (such as boundaries between 
different types of formations) from geophysical data (such as wave travel times). 
There are strong constraints on how the latent functions (e.g.~rock properties) relate
 to the given observations, and these constraints are implemented in the forward model. 
 Clearly, in these types of applications, the goal is not only making good probabilistic predictions of the 
 observables but also having good estimates of the posterior over  the latent functions.

Both applications mentioned above,  standard machine learning tasks and inversion problems, 
present three key modeling and inference challenges when having Gaussian process (\gp) priors. 
The first challenge is scalability, as \gp{s} are notorious for their poor scalability as a function 
of the number of training points. The second challenge is multi-output and multi-task task learning, 
as required by problems such as multi-output regression, multi-class classification 
or inversion over a multi-layer geological structure (where each layer is a task). 
Finally, the  third challenge is that of dealing with non-linear likelihoods, for example 
in classification, regression with non-Gaussian noise, and seismic inversion,
 as the posterior over the latent functions is analytically intractable.  

In order to address these challenges, we build upon the work of  
\citet{steinberg-bonilla-nips-2014} who showed that it is possible to obtain
good posterior estimates, in models with Gaussian process priors and 
nonlinear likelihoods, using  approximations of the nonlinear  likelihood  
via a Taylor series expansion or via the Unscented Transform \citep{Julier2004}. 
They refer to the two  flavors of their approach as the 
extended Gaussian process (\egp) and the unscented Gaussian process (\ugp).
One of the fundamental reasons why such linearizations are effective is 
because of their locality and adaptivity, as they are 
constructed around the current posterior estimate, 
which is iteratively updated within a variational inference procedure.

While the method of \citet{steinberg-bonilla-nips-2014} is an
effective way to tackle nonlinearities in the likelihood, their approach  
does not deal with the other two challenges mentioned above, namely multi-task learning 
and scalability. Indeed their method is specific to single-output problems and 
inherits the cubic scalability of standard \gp models as a function of the number of 
training points. This scalability problem is exacerbated when formulating  multi-task 
our multiple-output settings, as  naively with $N$ data points and $M$ tasks their
computational cost would be $N^3M^3$ in time, 
which renders their approach impractical to large datasets.  

In this paper we propose a scalable multiple-output generalization  
of the method of \citet{steinberg-bonilla-nips-2014}.
We deal with multiple-output problems  by using 
affine transformations of the latent functions and 
achieve scalability by introducing random feature approximations 
 of the covariance function of the Gaussian processes, in the style of 
  \citet{rahimi-recht-nips-2007}, 
 all embedded into a variational inference framework. Since 
\citet{rahimi-recht-nips-2007} refer to their approach as 
Random Kitchen Sinks,  we will refer to our methods 
as extended and unscented kitchen sinks, when using 
the Taylor series approximation or the Unscented Transform approximation 
in the conditional likelihood respectively. 

Our approach naturally avoids the cubic scalability of the 
original \egp and \ugp methods \citep{steinberg-bonilla-nips-2014} as a function of the number 
of training points, with the complexity dominated by 
the inverse of the feature covariance of size $D$, which has a time complexity of
$\bigO(D^3)$. 

Our experiments, on small-scale synthetic nonlinear regression tasks and on 
a classification task on the \usps dataset, show that random feature approximations to the 
\egp and the \ugp can attain similar performance to the original methods, 
even when using a small number of features, hence reducing the complexity 
of inference significantly. Furthermore, experiments at a larger scale on 
the \mnist dataset, show that our algorithms are competitive with recently developed 
approaches for inference in \gp models, while the application of the \egp and \ugp to
this task is simply unfeasible. Finally, on a multi-task (joint) nonlinear seismic inversion  problem,
we show that our algorithms can recover accurate representations of the underlying 
geological structure and rock properties (seismic velocities) on simulated and real data.


\fix{TODO: need a couple more citations in the intro}
\fix{Discuss that people haven't actually evaluated the full posterior estimation using \rks approaches }

\fix{cite Andrew Wilson}
 



 






