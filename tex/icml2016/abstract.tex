\begin{abstract}

    In this paper we propose a scalable multiple-output generalization of
    unscented and extended Gaussian processes. These algorithms have been
    designed to incorporate general nonlinear transformations in their
    likelihood functions by linearizing them using a Taylor series or the
    Unscented Transform in a variational inference framework, in which we also
    optimise all model parameters and hyperparameters. We achieve scalability
    by using random feature approximations of Gaussian process covariance
    functions, such as Random Kitchen Sink kernel approximations. We compare
    the new generalized algorithms on the same datasets as the original
    single-task algorithms. We also demonstrate the new algorithms classifying
    MNIST digits -- a task too large for the original algorithms, and
    performing a seismic inversion to infer the subsurface geological
    structures -- which is inherently a multi-task problem.

\end{abstract}
