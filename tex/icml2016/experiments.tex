\section{Experiments}
%
In this section we describe the experiments carried out in order to assess 
the performance and the behavior of our extended kitchen sinks (\eks) and 
unscented kitchen sinks (\uks) methods. We first look at experiments on 
small-scale synthetic inversion problems and a binary classification task on 
the \usps dataset. As these experiments were also carried out by 
\citet{steinberg-bonilla-nips-2014} to evaluate the \egp and the \ugp, 
our first goal  is to investigate how well 
random kitchen sinks basis can approximate the original \egp and \ugp.
Additionally, we are interested in determining whether the complexity 
of the algorithms  can be reduced significantly by having a smaller number 
of basis functions than the number of training points.
%
\subsection{Synthetic Inversion Problem}
In this experiment we generate latent function values ($\latents$) from a Gaussian 
process with isotropic squared exponential covariance function 
having a signal variance  $\sigma^2_s = 0.8^2$ and a length-scale $\ell = 0.6$.
The inputs  $\ins \in \real{}$ are uniformly spaced between $[-2\pi, 2\pi]$ 
%
Table \ref{tab:toyinversions}
Figure \ref{fig:toyinversions}
%
\begin{table*}[h]
\caption{Performance of the \eks and \uks methods compared to their GP counterparts (\egp and \ugp) on a range of synthetic benchmarks. 
\gp is the corresponds to the GP analytical solution in the linear case.
\todo{PUT GP baseline}
\label{tab:toyinversions}
}

\begin{center}
\input{table-toy}
\end{center}
\end{table*}
%
\begin{figure*}
\centering
\begin{tabular}{c c c}
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-gstar} \\
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-gstar} \\
\end{tabular}
\caption{The performance of the \eks (top) and \uks (bottom)
on the synthetic inversion problems as a function of the number of features used. 
\label{fig:toyinversions}
}
\end{figure*}
%
%%%%%%%
\subsection{Binary Handwritten Digit Classification}
Figure \ref{fig:usps}

\begin{figure*}[h]
\centering
\begin{tabular}{c c}
\includegraphics[width=0.45\linewidth]{uspsData-NLP}  &
\includegraphics[width=0.45\linewidth]{uspsData-ERROR-RATE}  
\end{tabular}
\caption{The performance of the \eks and \uks (bottom) on the binary classification problem for the \usps dataset as a function of 
the number of  basis used. \egp and \ugp are the original (full) \gp models.
\label{fig:usps}
}
\end{figure*}
%
%%%%%%%%%
\subsection{Large Scale Classification}
Table \ref{fig:mnistBinary}
Here we show the results on \mnist dataset.
\begin{table}[h]
\caption{The performance of the models on the \mnist dataset for the 
task of classifying the even digits vs the odd digits.
\label{fig:mnistBinary}
}
\begin{tabular}{c c c c c}
\toprule
& \multicolumn{2}{c}{NLP} & \multicolumn{2}{c}{Error Rate} \\
& D = 1000 & D = 2000 & D = 1000 & D = 2000 \\
\midrule
\eks &  0.129 & 0.088 & 0.043 & 0.026 \\
\uks &  0.129 & 0.088 & 0.043 & 0.026 \\
\hmg &      \multicolumn{2}{c}{0.069}    &            \multicolumn{2}{c}{0.022}   \\
\dnb   &      \multicolumn{2}{c}{0.068}    &            \multicolumn{2}{c}{0.022}\\
\bottomrule
\end{tabular}
\end{table}

\hmg refers to \citet{hensman-et-al-aistats-2015} 
\dnb refers to \citet{dezfouli-bonilla-nips-2015}

\input{seismic}

