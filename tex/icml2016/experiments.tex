\section{Experiments}
%
In this section we describe the experiments carried out in order to assess 
the performance and the behavior of our extended kitchen sinks (\eks) and 
unscented kitchen sinks (\uks) methods. We first look at experiments on 
small-scale synthetic inversion problems and a binary classification task on 
the \usps dataset. As these experiments were also carried out by 
\citet{steinberg-bonilla-nips-2014} to evaluate the \egp and the \ugp, 
our first goal  is to investigate how well 
random kitchen sinks basis can approximate the original \egp and \ugp.
Additionally, we are interested in determining whether the complexity 
of the algorithms  can be reduced significantly by having a smaller number 
of basis functions than the number of training points.
%
\subsection{Synthetic Inversion Problem}
In this experiment we generate latent function values ($\latents$) from a
Gaussian process with isotropic squared exponential covariance function (having
a signal variance  $\sigma^2_s = 0.8^2$ and a length-scale $\ell = 0.6$) at
1000 input points, $\ins \in \real{}$, which are uniformly spaced between
$[-2\pi, 2\pi]$. We test our algorithms and the baselines (\ugp, \egp) with
five simple forward models; an identity function (linear), a 3rd order
polynomial with no cross terms (poly3), an exponential function, a sinusoid,
and a tangent function. We present the results of 5-fold cross validation (200
training, 800 testing) in Table \ref{tab:toyinversions}, and we also experiment
with the number of basis functions for the \eks and \uks in
Figure \ref{fig:toyinversions}. As in \citet{steinberg-bonilla-nips-2014} we
use standardized mean square error (SMSE) and negative log probability density
(NLPD) as the performance metrics.
%
\begin{table}[h]
\caption{Performance of the \eks and \uks methods compared to their GP counterparts (\egp and \ugp) on a range of synthetic benchmarks. 
\gp the corresponds to the GP analytical solution in the linear case.
\todo{PUT GP baseline}
\label{tab:toyinversions}
}

\begin{center}
\begin{scriptsize}
\input{table-toy}
\end{scriptsize}
\end{center}
\end{table}
%
\begin{figure*}
\centering
\begin{tabular}{c c c}
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-gstar} \\
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-gstar} \\
\end{tabular}
\caption{The performance of the \eks (top) and \uks (bottom)
on the synthetic inversion problems as a function of the number of features used. 
\label{fig:toyinversions}
}
\end{figure*}
%
%%%%%%%
\subsection{Binary Handwritten Digit Classification}
Figure \ref{fig:usps}

\begin{figure*}[h]
\centering
\begin{tabular}{c c}
\includegraphics[width=0.45\linewidth]{uspsData-NLP}  &
\includegraphics[width=0.45\linewidth]{uspsData-ERROR-RATE}  
\end{tabular}
\caption{The performance of the \eks and \uks (bottom) on the binary classification problem for the \usps dataset as a function of 
the number of  basis used. \egp and \ugp are the original (full) \gp models.
\label{fig:usps}
}
\end{figure*}
%
%%%%%%%%%
\subsection{Large Scale Classification}
Table \ref{fig:mnistBinary}
Here we show the results on \mnist dataset.
\begin{table}[h]
\caption{The performance of the models on the \mnist dataset for the 
task of classifying the even digits vs the odd digits.
\label{fig:mnistBinary}
}
\begin{tabular}{c c c c c}
\toprule
& \multicolumn{2}{c}{NLP} & \multicolumn{2}{c}{Error Rate} \\
& D = 1000 & D = 2000 & D = 1000 & D = 2000 \\
\midrule
\eks &  0.129 & 0.088 & 0.043 & 0.026 \\
\uks &  0.129 & 0.088 & 0.043 & 0.026 \\
\hmg &      \multicolumn{2}{c}{0.069}    &            \multicolumn{2}{c}{0.022}   \\
\dnb   &      \multicolumn{2}{c}{0.068}    &            \multicolumn{2}{c}{0.022}\\
\bottomrule
\end{tabular}
\end{table}

\hmg refers to \citet{hensman-et-al-aistats-2015} 
\dnb refers to \citet{dezfouli-bonilla-nips-2015}

\input{seismic}

