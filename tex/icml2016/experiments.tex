\section{Experiments}
%
In this section we describe the experiments carried out in order to assess 
the performance and the behavior of our extended kitchen sinks (\eks) and 
unscented kitchen sinks (\uks) methods. We first look at experiments on 
small-scale synthetic inversion problems and a binary classification task on 
the \usps dataset. As these experiments were also carried out by 
\citet{steinberg-bonilla-nips-2014} to evaluate the \egp and the \ugp, 
our first goal  is to investigate how well 
random kitchen sinks basis can approximate the original \egp and \ugp.
Additionally, we are interested in determining whether the complexity 
of the algorithms  can be reduced significantly by having a smaller number 
of basis functions than the number of training points.
%
\subsection{Synthetic Inversion Problem}
In this experiment we generate latent function values ($\latents$) from a
Gaussian process with isotropic squared exponential covariance function (having
a signal variance  $\sigma^2_s = 0.8^2$ and a length-scale $\ell = 0.6$) at
1000 input points, $\ins \in \real{}$, which are uniformly spaced between
$[-2\pi, 2\pi]$. We test our algorithms and the baselines (\ugp, \egp) with
five simple forward models; an identity function (linear), a 3rd order
polynomial with no cross terms (poly3), an exponential function, a sinusoid,
and a tangent function. We present the results of 5-fold cross validation (200
training, 800 testing) in Table \ref{tab:toyinversions}, and we also experiment
with the number of basis functions for the \eks and \uks in
Figure \ref{fig:toyinversions}. As in \citet{steinberg-bonilla-nips-2014} we
use standardized mean square error (SMSE) and negative log probability density
(NLPD) as the performance metrics. \todo{how many basis functions are used in
    table?}
%
\begin{table}[t]
\caption{Performance of the \eks and \uks methods compared to their GP counterparts (\egp and \ugp) on a range of synthetic benchmarks. 
\gp the corresponds to the GP analytical solution in the linear case.
\todo{PUT GP baseline}
\label{tab:toyinversions}
}

\begin{center}
\begin{small}
\input{table-toy}
\end{small}
\end{center}
\end{table}

In general, the \eks and \uks perform similarly or better than the \egp and
\ugp algorithms, except for a few anomalies (e.g. \egp with the exponential
forward model) that have associated high standard deviations, suggesting the
algorithms converged suboptimally in one or more of the folds. Interestingly,
this suggests that on these simple problems, the \eks and \uks can perform at
least as well as the \egp and \uks algorithms, but with significantly less
computational cost since $D < N$. The \eks appears to have more robust
performance than the \uks w.r.t.\ the number of basis functions used. Apart
from the \uks with 50 basis functions, the performance of these algorithms does
not seem to be a strong function of the number of bases. Since the input
dimension is low ($\Ins$ are 1-dimensional), and the latent functions and
forward models are fairly smooth, this is unsurprising.

\begin{figure*}
\centering
\begin{tabular}{c c c}
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-gstar} \\
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-gstar} \\
\end{tabular}
\caption{The performance of the \eks (top) and \uks (bottom)
on the synthetic inversion problems as a function of the number of features used. 
\label{fig:toyinversions}
}
\end{figure*}
%
%%%%%%%
\subsection{Binary Handwritten Digit Classification}
In this experiment we re-create the second experiment from
\citet{steinberg-bonilla-nips-2014}, which is a binary classification task to
classify between images of the handwritten digits `3' and `5' in the USPS
digits datasets \cite{rasmussen-williams-book}. There are 767 images in the
training set, and 773 images in the test set. We use a logistic sigmoid as a
forward model in this task and the same settings as in the original
    experiments for covariance functions and observation variance. The main
aim of this experiment, again, is to benchmark the performance of the \eks and
\uks against the \egp and \ugp for varying numbers of basis functions, as shown
in Figure \ref{fig:usps}. 

We observe that there is a detriment in performance with a small 
number of features ($D=200$),  which is to be expected since the input image 
features have a dimension of $d = 256$, and so
the exact covariance would be harder to represent with a low number of random
basis functions. 
However, the performance of the
\eks and \uks approaches that of the \ugp and \egp with 400 basis functions,
which indicates that our random-feature approaches are reasonable approximations to
the original \gp model. More importantly, for problems with a large number of training 
points, the \egp and the \ugp are simply unfeasible, and this is the subject of discussion 
in the next section.
\begin{figure}[t]
\centering
\begin{tabular}{c c}
\includegraphics[width=0.8\linewidth]{uspsData-NLP}  \\
\includegraphics[width=0.8\linewidth]{uspsData-ERROR-RATE}  
\end{tabular}
\caption{The performance of the \eks and \uks (bottom) on the binary classification problem for the \usps dataset as a function of 
the number of  basis used. \egp and \ugp are the original (full) \gp models.
\label{fig:usps}
}
\end{figure}
%
 
\input{mnistBinary}

\input{seismic}
 
