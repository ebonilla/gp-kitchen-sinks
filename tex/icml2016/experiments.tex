\section{Experiments}

\subsection{Synthetic Inversion Problem}
%
\begin{table*}
\caption{Performance of the \eks and \uks methods compared to their GP counterparts (\egp and \ugp) on a range of synthetic benchmarks. 
\gp is the corresponds to the GP analytical solution in the linear case.
}
\begin{center}
\input{table-toy}
\end{center}
\end{table*}
%
\begin{figure*}
\centering
\begin{tabular}{c c c}
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-EKS-SMSE-gstar} \\
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-NLPD-fstar} &
\includegraphics[width=0.31\linewidth]{toyData-UKS-SMSE-gstar} \\
\end{tabular}
\caption{The performance of the \eks (top) and \uks (bottom) as a function of the number of features used. }
\end{figure*}
%
\subsection{Binary Handwritten Digit Classification}
\begin{figure*}
\centering
\begin{tabular}{c c}
\includegraphics[width=0.45\linewidth]{uspsData-NLP}  &
\includegraphics[width=0.45\linewidth]{uspsData-ERROR-RATE}  
\end{tabular}
\caption{The performance of the \eks and \uks (bottom) on the binary classification problem for the \usps dataset as a function of 
the number of  basis used. \egp and \ugp are the original (full) \gp models.}
\end{figure*}
%
%
\subsection{Large Scale Classification}
Here we show the results on \mnist dataset.
\begin{table}
\caption{The performance of the models on the \mnist dataset for the 
task of classifying the even digits vs the odd digits.}
\begin{tabular}{c c c c c}
\toprule
& \multicolumn{2}{c}{NLP} & \multicolumn{2}{c}{Error Rate} \\
& D = 1000 & D = 2000 & D = 1000 & D = 2000 \\
\midrule
\eks &  0.129 & 0.088 & 0.043 & 0.026 \\
\uks &  0.129 & 0.088 & 0.043 & 0.026 \\
\hmg &      \multicolumn{2}{c}{0.069}    &            \multicolumn{2}{c}{0.022}   \\
\dnb   &      \multicolumn{2}{c}{0.068}    &            \multicolumn{2}{c}{0.022}\\
\bottomrule
\end{tabular}
\end{table}

\hmg refers to \citet{hensman-et-al-aistats-2015} 
\dnb refers to \citet{dezfouli-bonilla-nips-2015}


\subsection{Seismic Inversion}

Seismic data from the Otway basin (Pretty Hill formation) is used in this
experiment. We wish to infer the geometry and layer velocity based on seismic
reflection data, and a prior on the \emph{mean depth} of the layers (constant
mean function in a GP). There are four layers in this dataset, which means $P =
4$ (travel times) and $Q = 8$ (depth of layer boundaries, layer velocities).
Will we also need a (non-zero) prior on velocities since we have more latent
tasks than output tasks??

This can be the experimental setup;
\begin{itemize}
    \item We can validate our algorithm on 1D slices of this data (about
        100--300 interpolated seismic travel times).
    \item We will use a 1D MCMC result as `ground truth' for this problem.
    \item We need to validate on 1D because MCMC cannot optimise too many knot
        points for the splines it uses to model the geometry (dimensionality is
        too high).
    \item We can also potentially compare to the simple optimisation method
        (non-probabilistic), especially if we also need to get the gradient of
        the forward model.
    \item We can then demonstrate the \eks and \uks on the whole 2D problem,
        which is inferring a $500\times300$ grid (O(100,000) points) \emph{per
            task}. We will need an SGD/SVI implementation for this.
\end{itemize}
