%%%%%%%%%
\subsection{Large Scale Classification} 
Here we present  results  on a larger application on the 
\mnist dataset, which contains examples of handwritten digits, 50,000 for training, 10,000 
for validation and 10,000 for testing. In our experiments, we always train on 60,000 
examples that include the training and the validation set and tune the parameters of our 
models via optimization of the variational bound. This is probably a disadvantage when compared
to other approaches that use cross-validation but our goal is only to show that our models can 
achieve competitive performance at this scale. 

\textbf{Odd digits vs even digits.} We first consider the binary classification problem 
of distinguishing the odd digits from the even digits, a task that has also been investigated by \citet{hensman-et-al-aistats-2015}. 
The results are shown in Table \ref{tab:mnistBinary}, when using 
a logistic sigmoid function as the forward model in our methods (\eks and \uks) and 
the methods by  \citet{hensman-et-al-aistats-2015} and \citet{dezfouli-bonilla-nips-2015}. 
We refer to these methods as \hmg and \dnb respectively.
As before, we report the mean negative log probability and the error rate on the test 
set for different number of features. We see that our methods achieve similar performance 
to \hmg and \dnb when using 2000 features. While \dnb is an inducing-point approach that 
uses 2000 inducing points fixed via clustering,   \hmg uses 200 inducing points with the 
extra overhead of learning their locations. Overall, we conclude that random feature 
approximations in our extended and unscented models are competitive with the 
state-of-the-art approaches to sparse \gp{s}.
%
\begin{table}[h]
\caption{The performance of the models on the \mnist dataset for the 
task of classifying the even digits vs the odd digits.
\label{tab:mnistBinary}
}
\begin{tabular}{c c c c c}
\toprule
& \multicolumn{2}{c}{NLP} & \multicolumn{2}{c}{Error Rate} \\
& D = 1000 & D = 2000 & D = 1000 & D = 2000 \\
\midrule
\eks &  0.129 & 0.088 & 0.043 & 0.026 \\
\uks &  0.129 & 0.088 & 0.043 & 0.026 \\
\hmg &      \multicolumn{2}{c}{0.069}    &            \multicolumn{2}{c}{0.022}   \\
\dnb   &      \multicolumn{2}{c}{0.068}    &            \multicolumn{2}{c}{0.022}\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Multi-class classification:} One of the contributions of our approach with respect 
to the original \egp and \ugp models is its scalability to a large number of observations 
when having multiple outputs. In this experiment we applied our algorithm to the task
of classifying all digits on \mnist using a softmax forward model. Our methods (\eks 
and \uks) achieved similar performance. For 
example, the \eks obtained an error rate of $ 4.75\%$ and 
an NLP of $-0.1887$, when using $D=1000$ features. When we increased the number 
of features to 	$D=2000$, it attained an error rate of 3.81\% and an NLP of $-0.1304$.
These error rates  are lower than that reported by  \citet{gal-et-al-nips-2014} of $5.95\%$, 
while  \citet{dezfouli-bonilla-nips-2015} reported an error 
rate of $2.51\%$. As a reference, linear classifiers achieve around $12\%$ error rate on this 
task while the state of the art is less than $1\%$. 

 

 

 




