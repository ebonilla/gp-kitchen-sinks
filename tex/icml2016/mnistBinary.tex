%%%%%%%%%
\subsection{Large Scale Classification} 
Here we present the results of our inference algorithms on a larger application on the 
\mnist dataset, which contains examples of handwritten digits, 50,000 for training, 10,000 
for validation and 10,000 for testing. In our experiments below, we always train on 60,000 
examples that include the training and the validation set and tune the parameters of our 
models via optimization of the variational bound. This is probably a disadvantage when compare
to other approaches that use cross-validation but our goal is only to show that our models can 
achieve competitive performance at this scale. 

\textbf{Odd digits vs even digits.} We first consider the binary classification problem 
of distinguishing the odd digits vs the 
even digits, a task that has also been investigated by \cite{hensman-et-al-aistats-2015}.
The results are shown in Table \ref{tab:mnistBinary}, for our methods (\eks and \uks) and 
the methods by  \citep[][\hmg]{hensman-et-al-aistats-2015} and \citep[][\dnb]{dezfouli-bonilla-nips-2015}.
As before, we report the mean negative log probability and the error rate on the test 
set for different number of feature basis. We see that our methods achieve similar performance 
to \hmg and \dnb when using 2000 feature basis, while \dnb is an inducing-point approach that 
uses 2000 inducing points fixed via clustering, and  \hmg uses 200 inducing points with the 
extra overhead of learning their locations. 
%
\begin{table}[h]
\caption{The performance of the models on the \mnist dataset for the 
task of classifying the even digits vs the odd digits.
\label{tab:mnistBinary}
}
\begin{tabular}{c c c c c}
\toprule
& \multicolumn{2}{c}{NLP} & \multicolumn{2}{c}{Error Rate} \\
& D = 1000 & D = 2000 & D = 1000 & D = 2000 \\
\midrule
\eks &  0.129 & 0.088 & 0.043 & 0.026 \\
\uks &  0.129 & 0.088 & 0.043 & 0.026 \\
\hmg &      \multicolumn{2}{c}{0.069}    &            \multicolumn{2}{c}{0.022}   \\
\dnb   &      \multicolumn{2}{c}{0.068}    &            \multicolumn{2}{c}{0.022}\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Multi-class classification:} One of the contributions of our approach with respect 
to the original \egp and \ugp models is its scalability to a large number of observations 
when having multiple outputs. In this experiment we applied our algorithm to the task
of classifying all digits on and the \eks and \uks achieved similar performance. For 
example, the \eks achieved and error rate of $ 04.75\%$ and 
a NLP of $-0.1887$, when using $D=1000$ features. When we increased the number 
of features to 	$D=2000$, it attained an error rate of 3.81\% and an NLP of $-0.1304$.
The error rates  lower than those reported by  \citet{gal-et-al-nips-2014}  and, with less 
computations, while  \citet{dezfouli-bonilla-nips-2015} reported an error 
rate of $2.51\%$. As a reference, linear classifiers achive around $12\%$ error rate on this 
task while the state of the art is less than $1\%$. 










