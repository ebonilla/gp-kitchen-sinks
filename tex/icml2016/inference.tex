\subsection{Posterior Approximation}
 Let us now make the simplifying assumption that posterior factorizes over 
 latent functions and has the
form,
\begin{equation}
	\label{eq:posteriorW}
    \qrob{\Weights} = \prod^Q_{q=1} \gausC{\weights_q}{\pweights_q, \pocov_q}.
\end{equation}
We can use variation inference to learn this posterior approximation, and
thereby allowing us to infer the posterior latent tasks,
\begin{equation}
\label{eq:posteriorF}
\qrob{\Latents}  = \prod^Q_{q=1} \gausC{\latentsq}{\Feats\pweights_q, \Feats \pocov_q  \Feats^T} \text{.}
%$\expectation[\latents_q] \approx \Feats\pweights_q$ and 
%$\covariance [\latents_q] \approx \Feats \pweights_q  \Feats^T$.
\end{equation}
%
%
\subsection{Evidence Lower Bound}
%
With the prior specified in Equation \eqref{eq:prior}; the likelihood 
specified in Equation \eqref{eq:likelihood}; and the approximate 
posterior defined in Equation \eqref{eq:posteriorW}, we are now ready 
to write down the variational lower bound that we aim to maximize 
in order to learn the parameters of our model.
%
The variational evidence lower bound is,
\begin{equation}
    \Fengy = \expec{\qrobsym\Weights}{\log \probC{\Outs}{\Weights, \Ins}}
        - \KL{\qrob{\Weights}}{\prob{\Weights}}.
\end{equation}
Here we can straight-forwardly arrive at the KL term,
\begin{multline}
    \KL{\qrob{\Weights}}{\prob{\Weights}}  = 
        \frac{1}{2} \sum^\ltdim_{q=1}
        \bigg[
        \frac{1}{\prvar_q}\trace{\pocov_q}
        + \frac{1}{\prvar_q} \pweights_q\transpose\pweights_q \\
         - \log\deter{\pocov_q} + D \log \prvar_q - D
        \bigg].
    \label{eq:KL}
\end{multline}
%
The expected log-likelihood term is less straight forward,
\begin{multline}
    \expec{\qrobsym\Weights}{\log \probC{\Outs}{\Weights, \Ins}}  = 
    - \frac{N}{2}
    \sbrac{\log{2\pi} + \log\deter{\Lcov}}  \\
    - \frac{1}{2} \sum^N_{n=1} \expec{\qrobsym\Weights}{
        \brac{\outs_n - \nonlin{\Weights\feats_n}}\transpose\Lcov\inv
        \brac{\outs_n - \nonlin{\Weights\feats_n}}},
    \label{eq:ell_exact}
\end{multline}
since this expectation cannot be easily evaluated. We make another
approximation,
\begin{equation}
    \nonlin{\Weights_n\feats_n} \approx \Linmat_n \Weights\feats_n + \intcpt_n,
    \label{eq:linapprox}
\end{equation}
where $\Linmat_n \in \real{\otdim \times \ltdim}$ is some linearization matrix
that we will define later, and $\intcpt_n \in \real{\otdim}$ is an intercept
term that we will also define later. Now we can evaluate the expectation in 
\eqref{eq:ell_exact} as approximately,
\begin{multline}
    \expec{\qrobsym\Weights}{\log \probC{\Outs}{\Weights, \Ins}} \approx
    - \frac{N}{2}\sbrac{P\log{2\pi} + \log\deter{\Lcov}} \\
    - \frac{1}{2} \sum^N_{n=1} 
    \sbrac{
        \errorn \transpose  \Lcov\inv   \errorn  
        + \sum^Q_{q=1} 
      %  \trace{\feats_n\Linvec_{nq}\transpose\Lcov\inv \Linvec_{nq}\feats_n\transpose\pocov_q} 
            \feats_n\transpose\pocov_q  \feats_n\Linvec_{nq}\transpose\Lcov\inv \Linvec_{nq}
        }.
    \label{eq:ell_approx}
\end{multline}
Where we have defined $\errorn \defeq  \outs_n - (\Linmat_n\pWeights\feats_n + \intcpt_n) $; 
 used $\Linmat_n\Weights = \sum_q \Linvec_{nq}
\weights_q\transpose$ defining
$\Linvec_{nq} \in \real{P}$ and $\Linmat_n = \sbrac{\Linvec_{n1}, \ldots,
    \Linvec_{nQ}}$. Here we can see that this objective easily factorizes over the
data, and so it should be straight forward to apply parallel or stochastic
gradient descent algorithms to learn the posterior parameters.


\subsection{Learning the Variational Parameters}

As in \citet{steinberg-bonilla-nips-2014}, we can use Newton's method to learn
the approximate posterior parameters for each task,
\begin{equation}
    \pweights_{q}^{(k+1)} = \pweights_{q}^{(k)} - \step_k
        \brac{\nabla_{\pweights_q} \nabla_{\pweights_q} \Fengy}\inv
        \nabla_{\pweights_q} \Fengy ~ \Big|_{\pweights_q = \pweights_{q}^{(k)}}.
        \label{eq:newt}
\end{equation}
Here $\step_k \in (0, 1]$ is a step length, and the gradients of the 
variational lower bound with respect to the posterior mean are:
\begin{equation}
    \nabla_{\pweights_q} \Fengy = \sum^N_{n=1} \feats_n \Linvec_{nq}\transpose
        \Lcov\inv
        \brac{\outs_n - \Linvec_{nq}\pweights_q\transpose\feats_n - \intcpt_n}  \\
        - \frac{1}{\prvar_q} \pweights_q \text{.}
 \end{equation}     
%
Similarly, the Hessian of the variational objective is: 
\begin{equation}     
    \nabla_{\pweights_q} \nabla_{\pweights_q} \Fengy =
       - \frac{1}{\prvar_q}  \ident{D}
        - \sum^N_{n=1} \feats_n\Linvec_{nq}\transpose\Lcov\inv
        \Linvec_{nq}\feats_n\transpose.
\end{equation}
When \eqref{eq:newt} has converged to $\pweights_q\opt$ we can calculate the
approximate posterior covariance,
\begin{equation}
    \pocov_q 
%    =  - \brac{\nabla_{\pweights_q} \nabla_{\pweights_q} \Fengy}\inv ~ 
 %       \Big|_{\pweights_q = \pweights_q\opt}
    = \sbrac{ \frac{1}{\prvar_q} \ident{D}
        + \sum^N_{n=1} \feats_n\Linvec_{nq}\transpose\Lcov\inv
        \Linvec_{nq}\feats_n\transpose}\inv.
\end{equation}
Following \citet{steinberg-bonilla-nips-2014}, to asses convergence of
$\pweights_q\opt$ we linearize the forward models in the lower bound, \Fengy,
using a Taylor series approximation. This is equivalent to optimizing the
\emph{maximum a-posteriori} objective.
